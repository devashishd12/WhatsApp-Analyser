{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Author: Devashish Deshpande <ashu.9412@gmail.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import NaN, Inf, arange, isscalar, asarray, array, std\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as cl\n",
    "import operator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Email yourself the whatsapp conversation. Place it in the same directory as this code.\n",
    "   Change filename variable to whatever you've named your file.\n",
    "\"\"\"\n",
    "filename = raw_input(\"Enter filename with extension: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import stopwords. Stopwords taken from Google stopwords dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Purpose is to eliminate meaningless words such as 'the', 'a' etc. which might \n",
    "   spoil our analysis results.\n",
    "\"\"\"\n",
    "stopwords = []\n",
    "for l in open('stop-words.txt'):\n",
    "    stopwords.append(l.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    \n",
    "    Args:\n",
    "        string (str): input string\n",
    "        \n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    return filter(lambda s: s if s not in stopwords else '', re.split('\\W+', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    string = re.sub('[^a-zA-Z\\d\\s]+', '', text)\n",
    "    return string.lower().strip(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitData(line):\n",
    "    \"\"\"Parses the line to return the individual components of it.\n",
    "    TODO: Make it compatible with USA, China etc. date formats.\n",
    "    \n",
    "    Args:\n",
    "        line (str): Line of the form eg. '17/07/2015, 10:25 AM - (some string)'\n",
    "        Note that (some string) need not be of the form 'Name: Message'. Could be something like 'You were added'.\n",
    "        \n",
    "    Returns:\n",
    "        date: Date of message.\n",
    "        hour: Hour of message.\n",
    "        minute: Minute of message.\n",
    "        name: Name of texter.\n",
    "        text: Text of message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        timestamp, string = line.split('-', 1)\n",
    "    except ValueError:\n",
    "        print line\n",
    "    date, time = map(lambda x: x.strip(), timestamp.split(','))\n",
    "    \n",
    "    # In many convos, the time is already in 24 Hr format. This is handled here.\n",
    "    if 'AM' in time or 'PM' in time:\n",
    "        hour, minmeridiem = time.split(':')\n",
    "        minute, meridiem = minmeridiem.split()\n",
    "        # Converting hours to 24 Hr format.\n",
    "        if meridiem == 'AM' and int(hour) == 12:\n",
    "            hour = int(hour) - 12\n",
    "        elif meridiem == 'PM' and int(hour) != 12:\n",
    "            hour = int(hour) + 12\n",
    "        else:\n",
    "            hour = int(hour)\n",
    "    else:\n",
    "        hour, minute = map(lambda l: int(l), time.split(':'))\n",
    "        \n",
    "    # Handling strings of the form:\n",
    "    # 17/07/2015, 10:25 AM - You were added\n",
    "    # or\n",
    "    # 17/07/2015, 10:25 AM - Name created group “Group 1”\n",
    "    try:\n",
    "        name, text = map(lambda x: x.strip(), string.split(':', 1))\n",
    "        # Handling unsaved numbers.\n",
    "        if '\\xe2\\x80\\xaa' in name:\n",
    "            name=re.findall('\\+[0-9 ]+', name)[0]\n",
    "    except ValueError, e:\n",
    "        name = ''\n",
    "        text = string.strip()\n",
    "    # for media files\n",
    "    if '<Media omitted>' in text:\n",
    "        text='<Media omitted>'\n",
    "    else:\n",
    "        text = removePunctuation(text) # Problem might arise during sentiment analysis. Emoticons might be needed.\n",
    "    return (date, hour, minute, name, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reqd dictionaries to maintain track of each message.\n",
    "date = {}\n",
    "hour = {}\n",
    "minute = {}\n",
    "name = {}\n",
    "text = {}\n",
    "# and a list\n",
    "words = []\n",
    "multimedia = 0 # count of mulimedia messages.\n",
    "i=0 # Essentially a message id. Primary key to address all attributes\n",
    "for l in open(filename):\n",
    "    if re.search('\\d+/\\d+/\\d+, \\d+:\\d+',l) is not None:\n",
    "        splitData(l)\n",
    "        try:\n",
    "            date[i], hour[i], minute[i], name[i], text[i] = splitData(l)\n",
    "            if text[i] == '<Media omitted>':\n",
    "                multimedia += 1\n",
    "                text[i] = '123456789media_omitted123456789' # my little media message identification.\n",
    "        except ValueError:\n",
    "            print l\n",
    "        i += 1\n",
    "    else:\n",
    "        # Missing timestamp. Is a multiline message. Append to last texter's text. Will have to keep same timestamp\n",
    "        string = removePunctuation(l)\n",
    "        try:\n",
    "            text[i-1]+=' '+string\n",
    "        except KeyError, e:\n",
    "            print e\n",
    "        words.extend(tokenize(string))\n",
    "\n",
    "assert len(date) == len(name), 'Some problem in parsing'\n",
    "\n",
    "print \"No of messages we're analysing: %d \\nNo of multimedia messages: %d\" %(len(date),multimedia)\n",
    "# Tokenize rest of the text.\n",
    "for k in text.keys():\n",
    "    words.extend(tokenize(text[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some stuff with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for w in words:\n",
    "    if w in words_dict:\n",
    "        words_dict[w]+=1\n",
    "    else:\n",
    "        words_dict[w]=1\n",
    "del words_dict['123456789media_omitted123456789']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It. Is. Plotting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(words_dict.values(),color='blue')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "_ = plt.xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_words_list = sorted(words_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print 'Top 20 most used words:'\n",
    "for i in range(20):\n",
    "    print '{0} : {1}' .format(sorted_words_list[i][0], sorted_words_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak detection using sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/endolith/250860\n",
    "def peakdet(v, delta, x = None):\n",
    "    \"\"\"Implementation of peak detection using sliding window. \n",
    "    \n",
    "    Args: \n",
    "        v: 1-D data.\n",
    "        delta: Difference we want between peaks.\n",
    "          \n",
    "    Returns:\n",
    "        maxtab: np.array of peaks.\n",
    "        mintab: np.array of minimas. (Won't have any in our case.)\n",
    "    \"\"\"\n",
    "    maxtab = []\n",
    "    mintab = []\n",
    "       \n",
    "    if x is None:\n",
    "        x = arange(len(v))\n",
    "    \n",
    "    v = asarray(v)\n",
    "    \n",
    "    if len(v) != len(x):\n",
    "        sys.exit('Input vectors v and x must have same length')\n",
    "    \n",
    "    if not isscalar(delta):\n",
    "        sys.exit('Input argument delta must be a scalar')\n",
    "    \n",
    "    if delta <= 0:\n",
    "        sys.exit('Input argument delta must be positive')\n",
    "    \n",
    "    mn, mx = Inf, -Inf\n",
    "    mnpos, mxpos = NaN, NaN\n",
    "    \n",
    "    lookformax = True\n",
    "    \n",
    "    for i in arange(len(v)):\n",
    "        this = v[i]\n",
    "        if this > mx:\n",
    "            mx = this\n",
    "            mxpos = x[i]\n",
    "        if this < mn:\n",
    "            mn = this\n",
    "            mnpos = x[i]\n",
    "        \n",
    "        if lookformax:\n",
    "            if this < mx-delta:\n",
    "                maxtab.append((mxpos, mx))\n",
    "                mn = this\n",
    "                mnpos = x[i]\n",
    "                lookformax = False\n",
    "        else:\n",
    "            if this > mn+delta:\n",
    "                mintab.append((mnpos, mn))\n",
    "                mx = this\n",
    "                mxpos = x[i]\n",
    "                lookformax = True\n",
    "\n",
    "    return array(maxtab), array(mintab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"You might experience a problem here.\n",
    "   The window size is adjusted manually and depends on how the text is distributed.\n",
    "   This distribution varies among different conversations. \n",
    "   Tried np.std() but didn't work.\n",
    "\"\"\"\n",
    "maxtab, mintab = peakdet(words_dict.values(),100) #Adjust for delta manually to see peaks.\n",
    "plt.plot(words_dict.values(),color='blue')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks([])\n",
    "plt.scatter(maxtab[:,0],maxtab[:,1],color='r')\n",
    "print 'These are the peaks:'\n",
    "for i in maxtab[:,0]:\n",
    "    print '{0} : {1}'.format(words_dict.keys()[i], words_dict.values()[i])\n",
    "    plt.annotate(s=words_dict.keys()[i], xy=(i,words_dict.values()[i]), xytext=(10,10), \n",
    "                 textcoords='offset points', arrowprops=dict(arrowstyle=\"->\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Talkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We'll be using the name dictionary. Values are stored as {id:'name'} where id is 0,1,2...\n",
    "fracs=[]\n",
    "dictNames = {}\n",
    "for names in name.values():\n",
    "    if names in dictNames:\n",
    "        dictNames[names]+=1\n",
    "    else:\n",
    "        dictNames[names]=1\n",
    "\n",
    "if '' in dictNames.keys():\n",
    "    del dictNames[''] #eliminate empty names. Messages like \"You changed this group's icon\"\n",
    "sortedNames = sorted(dictNames.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for x in sortedNames: \n",
    "    print '{0} : {1} \\t{2:.2f}% messages'.format(x[0],x[1],(float(x[1])/sum(dictNames.values()))*100)\n",
    "\n",
    "#Bar graph can get screwed up. Uncomment at your own risk.\n",
    "\n",
    "#plt.bar(range(len(sortedNames)), map(lambda l: l[1], sortedNames))\n",
    "#plt.xticks(map(lambda l: l+0.5,range(len(sortedNames))),map(lambda l: l[0][:3],sortedNames))\n",
    "\n",
    "_ = plt.pie(x=dictNames.values(),labels=dictNames.keys(), labeldistance=1.03, radius=3, shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we'll be using the hour dictionary\n",
    "dictHours = {}\n",
    "for hours in hour.values():\n",
    "    if hours in dictHours:\n",
    "        dictHours[hours]+=1\n",
    "    else:\n",
    "        dictHours[hours]=1\n",
    "dictHours = dict(sorted(dictHours.items(),key=operator.itemgetter(0)))\n",
    "for i in range(24):\n",
    "    if i not in dictHours.keys():\n",
    "        dictHours[i] = 0\n",
    "for x in dictHours: print '{0} : {1}' .format(x, dictHours[x])\n",
    "mostActive = sorted(dictHours.items(),key=operator.itemgetter(1),reverse=True)\n",
    "print 'Most active around: {0} hrs with {1} messages'.format(mostActive[0][0],mostActive[0][1])\n",
    "plt.plot(dictHours.values())\n",
    "plt.xticks(range(24))\n",
    "_ = plt.xlabel('Hour of the day')\n",
    "_ = plt.ylabel('No of messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequently used words by individual participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Using dictName.key() to get the participants.\n",
    "   In the end our data will be stored like {'participant':{'word1':100,'word2':120}, 'participant2':{'word3':40}}\n",
    "\"\"\"\n",
    "partDict = {}\n",
    "for i in range(len(date)):\n",
    "    if text[i]!='123456789media_omitted123456789' and name[i]!='':\n",
    "        if name[i] in partDict.keys():\n",
    "            partDict[name[i]].extend(tokenize(text[i]))\n",
    "        else:\n",
    "            partDict[name[i]]=tokenize(text[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for k in partDict.keys():\n",
    "    partWords = {}\n",
    "    for w in partDict[k]:\n",
    "        if w in partWords:\n",
    "            partWords[w]+=1\n",
    "        else:\n",
    "            partWords[w]=1\n",
    "    partDict[k]=partWords\n",
    "\n",
    "print 'Most used word by:'\n",
    "\n",
    "for k in partDict.keys():\n",
    "    sorted_part_List=sorted(partDict[k].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    try:\n",
    "        print '{0} is:\\n\\t {1} : {2}'.format(k,sorted_part_List[0][0],sorted_part_List[0][1])\n",
    "    except IndexError:\n",
    "        print '{0}:\\n\\tno words yet.'.format(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mod function for cosine similarity__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mod(A, f_max, idf):\n",
    "    \"\"\"This function calculates the mod of the values\n",
    "    of the keys.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    A : List of values of keys like [1, 4, 2, 3, ....].\n",
    "    \n",
    "    f_max : int frequency of maximum occuring word.\n",
    "    \n",
    "    idf : float inverse document frequency of name.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Float value corresponding to the mod of the given list.\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, dtype=np.float64)\n",
    "    A = A / f_max\n",
    "    A = A * idf\n",
    "    return math.sqrt(A.dot(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cosine similarity function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(name1, name2):\n",
    "    \"\"\"This function calculates cosine similarity between two\n",
    "    dictionaries for the purpose of analysing typing similarity.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    name1 : First name.\n",
    "    \n",
    "    name2 : Second name.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float value corresponding to the cosine similarity.\n",
    "    \"\"\"\n",
    "    A = partDict[name1]\n",
    "    B = partDict[name2]\n",
    "    f_max = sorted_words_list[0][1]\n",
    "    a = set(A.keys())\n",
    "    b = set(B.keys())\n",
    "    mat1 = np.array([])\n",
    "    mat2 = np.array([])\n",
    "    dotprod = 0\n",
    "    for word in a.intersection(b):\n",
    "        mat1 = np.append(mat1, A[word])\n",
    "        mat2 = np.append(mat2, B[word])\n",
    "    mat1 = mat1 / f_max\n",
    "    mat2 = mat2 / f_max\n",
    "    N = sum(dictNames.values())\n",
    "    idf1 = np.log2(N / dictNames[name1])\n",
    "    idf2 = np.log2(N / dictNames[name2])\n",
    "    mat1 = mat1 * idf1\n",
    "    mat2 = mat2 * idf2\n",
    "    \n",
    "    return mat1.dot(mat2)/(mod(A.values(), f_max, idf1) * mod(B.values(), f_max, idf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Typing style similarity__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = partDict.keys()\n",
    "backtracker = {}\n",
    "for i in range(len(names)-1):\n",
    "    for j in range(i+1, len(names)):\n",
    "        backtracker[(names[i], names[j])] = cosine_similarity(names[i], names[j])\n",
    "for k in backtracker.keys():\n",
    "    print \"{0}, {1} : {2}\".format(k[0], k[1], backtracker[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation trends over full timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateDict={}\n",
    "\n",
    "for i in range(len(date)):\n",
    "    if name[i]!='':\n",
    "        if date[i] in dateDict:\n",
    "            dateDict[date[i]]+=1\n",
    "        else:\n",
    "            dateDict[date[i]]=1\n",
    "    else:\n",
    "        continue\n",
    "print 'Around %d messages everyday.' %(sum(dateDict.values())/len(dateDict.keys()))\n",
    "\n",
    "dateList=[]\n",
    "\n",
    "#Rearranging dates for easier comparisons.\n",
    "\n",
    "for i in range(len(dateDict)):\n",
    "    l = map(int,dateDict.keys()[i].split('/'))\n",
    "    #This date splitting is specific to DD/MM/YYYY format\n",
    "    dt=(l[2],l[1],l[0])\n",
    "    dateList.append((dt,dateDict.values()[i]))\n",
    "\n",
    "dateList = sorted(dateList)\n",
    "\n",
    "x_coords_datelist = map(lambda l: l[0],dateList)\n",
    "y_coords_datelist = map(lambda l: l[1],dateList)\n",
    "\n",
    "dateListRev = sorted(dateList,key=operator.itemgetter(1),reverse=True)\n",
    "maxList, minList = dateListRev[:5], dateListRev[::-1][:5]\n",
    "\n",
    "print 'Maximum messages:'\n",
    "for i in maxList:\n",
    "    string = '{0} : {1}'.format(i[0][::-1], i[1])\n",
    "    print string\n",
    "\n",
    "print 'Minimum messages:'\n",
    "for i in minList:\n",
    "    string = '{0} : {1}'.format(i[0][::-1], i[1])\n",
    "    print string\n",
    "#Getting coordinates for max 5 and min 5 days\n",
    "maxmsg=[]\n",
    "minmsg=[]\n",
    "for i in range(len(dateList)):\n",
    "    if dateList[i] in maxList:\n",
    "        maxmsg.append((i,dateList[i][1]))\n",
    "    elif dateList[i] in minList:\n",
    "        minmsg.append((i,dateList[i][1]))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(len(dateList)), y_coords_datelist)\n",
    "\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Messages')\n",
    "plt.xticks([i for i in range(0,len(dateList),10)],\n",
    "            [str(dateList[i][0][2])+'/'+str(dateList[i][0][1])+'/'+str(dateList[i][0][0]) for i in range(0,len(dateList),10)],\n",
    "            rotation=17)\n",
    "\n",
    "plt.scatter(map(lambda x: x[0],maxmsg),map(lambda x: x[1],maxmsg),color='r')\n",
    "for i in maxmsg:\n",
    "    string = '{0} : {1}'.format(dateList[i[0]][0][::-1], i[1])\n",
    "    plt.annotate(s=string, xy=(i[0],i[1]), xytext=(10,10), \n",
    "                 textcoords='offset points', arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.scatter(map(lambda x: x[0],minmsg),map(lambda x: x[1],minmsg),color='g')\n",
    "for i in minmsg:\n",
    "    string = '{0} : {1}'.format(dateList[i[0]][0][::-1], i[1])\n",
    "    plt.annotate(s=string, xy=(i[0],i[1]), xytext=(-10,-10), \n",
    "                 textcoords='offset points', arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "\n",
    "#Best fit line\n",
    "weights = np.polyfit(range(len(dateList)),y_coords_datelist,1)\n",
    "\n",
    "for x in range(len(dateList)):\n",
    "    plt.scatter(x, x*weights[0]+weights[1],marker='+',color='r')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
